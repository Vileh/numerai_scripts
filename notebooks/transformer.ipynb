{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q numerapi pandas lightgbm cloudpickle pyarrow scikit-learn scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "napi = NumerAPI()\n",
    "\n",
    "# use one of the latest data versions\n",
    "DATA_VERSION = \"v4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi.download_dataset(\"v4.3/train_int8.parquet\", \"v4.3/train_int8.parquet\")\n",
    "napi.download_dataset(\"v4.3/validation_int8.parquet\", \"v4.3/validation_int8.parquet\")\n",
    "napi.download_dataset(\"v4.3/live_int8.parquet\", \"v4.3/live_int8.parquet\")\n",
    "napi.download_dataset(\"v4.3/live_example_preds.parquet\", \"v4.3/live_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.3/validation_example_preds.parquet\", \"v4.3/validation_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.3/features.json\", \"v4.3/features.json\")\n",
    "napi.download_dataset(\"v4.3/meta_model.parquet\", \"v4.3/meta_model.parquet\")\n",
    "napi.download_dataset(\"v4.3/live_benchmark_models.parquet\", \"v4.3/live_benchmark_models.parquet\")\n",
    "napi.download_dataset(\"v4.3/validation_benchmark_models.parquet\", \"v4.3/validation_benchmark_models.parquet\")\n",
    "napi.download_dataset(\"v4.3/train_benchmark_models.parquet\", \"v4.3/train_benchmark_models.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "features = feature_metadata[\"feature_sets\"][\"medium\"] # use \"all\" for better performance. Requires more RAM.\n",
    "train = pd.read_parquet(f\"{DATA_VERSION}/train_int8.parquet\", columns=[\"era\"]+features+[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample for speed\n",
    "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]  # skip this step for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "      <th>feature_abating_unadaptable_weakfish</th>\n",
       "      <th>feature_ablest_mauritanian_elding</th>\n",
       "      <th>feature_acclimatisable_unfeigned_maghreb</th>\n",
       "      <th>feature_accommodable_crinite_cleft</th>\n",
       "      <th>feature_accretive_sorrier_skedaddle</th>\n",
       "      <th>feature_acetose_periotic_coronation</th>\n",
       "      <th>feature_adam_incantational_winemaker</th>\n",
       "      <th>feature_additive_untrustworthy_hierologist</th>\n",
       "      <th>feature_adsorbed_blizzardy_burlesque</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_wombed_liberatory_malva</th>\n",
       "      <th>feature_won_stalwart_eisenstein</th>\n",
       "      <th>feature_wrathful_prolix_colotomy</th>\n",
       "      <th>feature_wrinkliest_unmaintainable_usk</th>\n",
       "      <th>feature_wrought_muckier_temporality</th>\n",
       "      <th>feature_yauld_antediluvial_subprefecture</th>\n",
       "      <th>feature_yelled_hysteretic_eath</th>\n",
       "      <th>feature_yoruban_unapplied_tawse</th>\n",
       "      <th>feature_zygodactyl_exponible_lathi</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n003bba8a98662e4</th>\n",
       "      <td>0001</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bee128c2fcfc</th>\n",
       "      <td>0001</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0048ac83aff7194</th>\n",
       "      <td>0001</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n00691bec80d3e02</th>\n",
       "      <td>0001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n00b8720a2fdc4f2</th>\n",
       "      <td>0001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nffc2d5e4b79a7ae</th>\n",
       "      <td>0573</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nffc7d24176548a4</th>\n",
       "      <td>0573</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nffc9844c1c7a6a9</th>\n",
       "      <td>0573</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nffd79773f4109bb</th>\n",
       "      <td>0573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nfff87b21e4db902</th>\n",
       "      <td>0573</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>606176 rows Ã— 707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   era  feature_abating_unadaptable_weakfish   \n",
       "id                                                             \n",
       "n003bba8a98662e4  0001                                     0  \\\n",
       "n003bee128c2fcfc  0001                                     4   \n",
       "n0048ac83aff7194  0001                                     4   \n",
       "n00691bec80d3e02  0001                                     1   \n",
       "n00b8720a2fdc4f2  0001                                     0   \n",
       "...                ...                                   ...   \n",
       "nffc2d5e4b79a7ae  0573                                     4   \n",
       "nffc7d24176548a4  0573                                     0   \n",
       "nffc9844c1c7a6a9  0573                                     4   \n",
       "nffd79773f4109bb  0573                                     0   \n",
       "nfff87b21e4db902  0573                                     3   \n",
       "\n",
       "                  feature_ablest_mauritanian_elding   \n",
       "id                                                    \n",
       "n003bba8a98662e4                                  4  \\\n",
       "n003bee128c2fcfc                                  2   \n",
       "n0048ac83aff7194                                  4   \n",
       "n00691bec80d3e02                                  4   \n",
       "n00b8720a2fdc4f2                                  2   \n",
       "...                                             ...   \n",
       "nffc2d5e4b79a7ae                                  2   \n",
       "nffc7d24176548a4                                  3   \n",
       "nffc9844c1c7a6a9                                  1   \n",
       "nffd79773f4109bb                                  0   \n",
       "nfff87b21e4db902                                  3   \n",
       "\n",
       "                  feature_acclimatisable_unfeigned_maghreb   \n",
       "id                                                           \n",
       "n003bba8a98662e4                                         0  \\\n",
       "n003bee128c2fcfc                                         2   \n",
       "n0048ac83aff7194                                         2   \n",
       "n00691bec80d3e02                                         1   \n",
       "n00b8720a2fdc4f2                                         0   \n",
       "...                                                    ...   \n",
       "nffc2d5e4b79a7ae                                         4   \n",
       "nffc7d24176548a4                                         3   \n",
       "nffc9844c1c7a6a9                                         1   \n",
       "nffd79773f4109bb                                         1   \n",
       "nfff87b21e4db902                                         1   \n",
       "\n",
       "                  feature_accommodable_crinite_cleft   \n",
       "id                                                     \n",
       "n003bba8a98662e4                                   4  \\\n",
       "n003bee128c2fcfc                                   2   \n",
       "n0048ac83aff7194                                   0   \n",
       "n00691bec80d3e02                                   1   \n",
       "n00b8720a2fdc4f2                                   0   \n",
       "...                                              ...   \n",
       "nffc2d5e4b79a7ae                                   4   \n",
       "nffc7d24176548a4                                   4   \n",
       "nffc9844c1c7a6a9                                   2   \n",
       "nffd79773f4109bb                                   3   \n",
       "nfff87b21e4db902                                   1   \n",
       "\n",
       "                  feature_accretive_sorrier_skedaddle   \n",
       "id                                                      \n",
       "n003bba8a98662e4                                    2  \\\n",
       "n003bee128c2fcfc                                    2   \n",
       "n0048ac83aff7194                                    2   \n",
       "n00691bec80d3e02                                    2   \n",
       "n00b8720a2fdc4f2                                    2   \n",
       "...                                               ...   \n",
       "nffc2d5e4b79a7ae                                    3   \n",
       "nffc7d24176548a4                                    2   \n",
       "nffc9844c1c7a6a9                                    0   \n",
       "nffd79773f4109bb                                    1   \n",
       "nfff87b21e4db902                                    4   \n",
       "\n",
       "                  feature_acetose_periotic_coronation   \n",
       "id                                                      \n",
       "n003bba8a98662e4                                    0  \\\n",
       "n003bee128c2fcfc                                    3   \n",
       "n0048ac83aff7194                                    0   \n",
       "n00691bec80d3e02                                    0   \n",
       "n00b8720a2fdc4f2                                    0   \n",
       "...                                               ...   \n",
       "nffc2d5e4b79a7ae                                    1   \n",
       "nffc7d24176548a4                                    0   \n",
       "nffc9844c1c7a6a9                                    3   \n",
       "nffd79773f4109bb                                    4   \n",
       "nfff87b21e4db902                                    2   \n",
       "\n",
       "                  feature_adam_incantational_winemaker   \n",
       "id                                                       \n",
       "n003bba8a98662e4                                     2  \\\n",
       "n003bee128c2fcfc                                     2   \n",
       "n0048ac83aff7194                                     2   \n",
       "n00691bec80d3e02                                     2   \n",
       "n00b8720a2fdc4f2                                     2   \n",
       "...                                                ...   \n",
       "nffc2d5e4b79a7ae                                     0   \n",
       "nffc7d24176548a4                                     2   \n",
       "nffc9844c1c7a6a9                                     2   \n",
       "nffd79773f4109bb                                     2   \n",
       "nfff87b21e4db902                                     0   \n",
       "\n",
       "                  feature_additive_untrustworthy_hierologist   \n",
       "id                                                             \n",
       "n003bba8a98662e4                                           1  \\\n",
       "n003bee128c2fcfc                                           1   \n",
       "n0048ac83aff7194                                           4   \n",
       "n00691bec80d3e02                                           2   \n",
       "n00b8720a2fdc4f2                                           3   \n",
       "...                                                      ...   \n",
       "nffc2d5e4b79a7ae                                           0   \n",
       "nffc7d24176548a4                                           3   \n",
       "nffc9844c1c7a6a9                                           4   \n",
       "nffd79773f4109bb                                           1   \n",
       "nfff87b21e4db902                                           0   \n",
       "\n",
       "                  feature_adsorbed_blizzardy_burlesque  ...   \n",
       "id                                                      ...   \n",
       "n003bba8a98662e4                                     4  ...  \\\n",
       "n003bee128c2fcfc                                     3  ...   \n",
       "n0048ac83aff7194                                     1  ...   \n",
       "n00691bec80d3e02                                     1  ...   \n",
       "n00b8720a2fdc4f2                                     1  ...   \n",
       "...                                                ...  ...   \n",
       "nffc2d5e4b79a7ae                                     4  ...   \n",
       "nffc7d24176548a4                                     1  ...   \n",
       "nffc9844c1c7a6a9                                     0  ...   \n",
       "nffd79773f4109bb                                     2  ...   \n",
       "nfff87b21e4db902                                     1  ...   \n",
       "\n",
       "                  feature_wombed_liberatory_malva   \n",
       "id                                                  \n",
       "n003bba8a98662e4                                0  \\\n",
       "n003bee128c2fcfc                                3   \n",
       "n0048ac83aff7194                                1   \n",
       "n00691bec80d3e02                                1   \n",
       "n00b8720a2fdc4f2                                1   \n",
       "...                                           ...   \n",
       "nffc2d5e4b79a7ae                                1   \n",
       "nffc7d24176548a4                                0   \n",
       "nffc9844c1c7a6a9                                3   \n",
       "nffd79773f4109bb                                4   \n",
       "nfff87b21e4db902                                2   \n",
       "\n",
       "                  feature_won_stalwart_eisenstein   \n",
       "id                                                  \n",
       "n003bba8a98662e4                                2  \\\n",
       "n003bee128c2fcfc                                2   \n",
       "n0048ac83aff7194                                2   \n",
       "n00691bec80d3e02                                2   \n",
       "n00b8720a2fdc4f2                                2   \n",
       "...                                           ...   \n",
       "nffc2d5e4b79a7ae                                2   \n",
       "nffc7d24176548a4                                2   \n",
       "nffc9844c1c7a6a9                                0   \n",
       "nffd79773f4109bb                                1   \n",
       "nfff87b21e4db902                                1   \n",
       "\n",
       "                  feature_wrathful_prolix_colotomy   \n",
       "id                                                   \n",
       "n003bba8a98662e4                                 4  \\\n",
       "n003bee128c2fcfc                                 0   \n",
       "n0048ac83aff7194                                 0   \n",
       "n00691bec80d3e02                                 2   \n",
       "n00b8720a2fdc4f2                                 0   \n",
       "...                                            ...   \n",
       "nffc2d5e4b79a7ae                                 1   \n",
       "nffc7d24176548a4                                 4   \n",
       "nffc9844c1c7a6a9                                 3   \n",
       "nffd79773f4109bb                                 2   \n",
       "nfff87b21e4db902                                 2   \n",
       "\n",
       "                  feature_wrinkliest_unmaintainable_usk   \n",
       "id                                                        \n",
       "n003bba8a98662e4                                      2  \\\n",
       "n003bee128c2fcfc                                      2   \n",
       "n0048ac83aff7194                                      2   \n",
       "n00691bec80d3e02                                      2   \n",
       "n00b8720a2fdc4f2                                      2   \n",
       "...                                                 ...   \n",
       "nffc2d5e4b79a7ae                                      3   \n",
       "nffc7d24176548a4                                      2   \n",
       "nffc9844c1c7a6a9                                      0   \n",
       "nffd79773f4109bb                                      0   \n",
       "nfff87b21e4db902                                      2   \n",
       "\n",
       "                  feature_wrought_muckier_temporality   \n",
       "id                                                      \n",
       "n003bba8a98662e4                                    4  \\\n",
       "n003bee128c2fcfc                                    2   \n",
       "n0048ac83aff7194                                    3   \n",
       "n00691bec80d3e02                                    3   \n",
       "n00b8720a2fdc4f2                                    0   \n",
       "...                                               ...   \n",
       "nffc2d5e4b79a7ae                                    2   \n",
       "nffc7d24176548a4                                    0   \n",
       "nffc9844c1c7a6a9                                    2   \n",
       "nffd79773f4109bb                                    1   \n",
       "nfff87b21e4db902                                    3   \n",
       "\n",
       "                  feature_yauld_antediluvial_subprefecture   \n",
       "id                                                           \n",
       "n003bba8a98662e4                                         3  \\\n",
       "n003bee128c2fcfc                                         1   \n",
       "n0048ac83aff7194                                         2   \n",
       "n00691bec80d3e02                                         2   \n",
       "n00b8720a2fdc4f2                                         1   \n",
       "...                                                    ...   \n",
       "nffc2d5e4b79a7ae                                         2   \n",
       "nffc7d24176548a4                                         2   \n",
       "nffc9844c1c7a6a9                                         0   \n",
       "nffd79773f4109bb                                         1   \n",
       "nfff87b21e4db902                                         1   \n",
       "\n",
       "                  feature_yelled_hysteretic_eath   \n",
       "id                                                 \n",
       "n003bba8a98662e4                               2  \\\n",
       "n003bee128c2fcfc                               3   \n",
       "n0048ac83aff7194                               1   \n",
       "n00691bec80d3e02                               2   \n",
       "n00b8720a2fdc4f2                               1   \n",
       "...                                          ...   \n",
       "nffc2d5e4b79a7ae                               2   \n",
       "nffc7d24176548a4                               1   \n",
       "nffc9844c1c7a6a9                               0   \n",
       "nffd79773f4109bb                               0   \n",
       "nfff87b21e4db902                               0   \n",
       "\n",
       "                  feature_yoruban_unapplied_tawse   \n",
       "id                                                  \n",
       "n003bba8a98662e4                                2  \\\n",
       "n003bee128c2fcfc                                2   \n",
       "n0048ac83aff7194                                2   \n",
       "n00691bec80d3e02                                2   \n",
       "n00b8720a2fdc4f2                                2   \n",
       "...                                           ...   \n",
       "nffc2d5e4b79a7ae                                1   \n",
       "nffc7d24176548a4                                2   \n",
       "nffc9844c1c7a6a9                                1   \n",
       "nffd79773f4109bb                                1   \n",
       "nfff87b21e4db902                                1   \n",
       "\n",
       "                  feature_zygodactyl_exponible_lathi  target  \n",
       "id                                                            \n",
       "n003bba8a98662e4                                   3    0.25  \n",
       "n003bee128c2fcfc                                   1    0.75  \n",
       "n0048ac83aff7194                                   2    0.25  \n",
       "n00691bec80d3e02                                   2    0.75  \n",
       "n00b8720a2fdc4f2                                   1    0.50  \n",
       "...                                              ...     ...  \n",
       "nffc2d5e4b79a7ae                                   1    0.25  \n",
       "nffc7d24176548a4                                   3    0.50  \n",
       "nffc9844c1c7a6a9                                   2    0.50  \n",
       "nffd79773f4109bb                                   0    0.50  \n",
       "nfff87b21e4db902                                   3    0.50  \n",
       "\n",
       "[606176 rows x 707 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim).to(device)\n",
    "    self.beta = torch.zeros(dim).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" single self-attention head\"\"\"\n",
    "\n",
    "    def __init__(self, block_size, n_embd, n_head, dropout, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, n_embd // n_head, bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd // n_head, bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd // n_head, bias)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        w = (q @ k.transpose(-2, -1)) * (int(c) ** -0.5)\n",
    "        w = w.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        a = w @ v\n",
    "        return a\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multi-head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, n_embd, n_head, dropout, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(block_size, n_embd, n_head, dropout, bias) for _ in range(n_head)])\n",
    "        self.linear = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        a = self.dropout(self.linear(a))\n",
    "        return a\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" simple positional feed-forward \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, n_embd, n_head, n_hidden, dropout, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(block_size, n_embd, n_head, dropout, bias)\n",
    "        self.ff = FeedForward(n_embd, n_hidden, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" a simple Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, block_size_out, n_embd, n_head, n_layer, n_vocab, n_hidden, dropout, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.block_size_out = block_size_out\n",
    "        self.token_emb = nn.Embedding(n_vocab, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(block_size, n_embd, n_head, n_hidden, dropout, bias) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = MLP(n_embd, n_hidden, block_size_out)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        b, t = x.shape\n",
    "        assert t <= self.block_size, \"x has to be within block size\"\n",
    "\n",
    "        print(x)\n",
    "        print(x.dtype)\n",
    "        x = self.token_emb(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        out = self.head(x)\n",
    "        print(out.shape)\n",
    "        if self.block_size_out is not None:\n",
    "            logits = logits[:, -self.block_size_out:, :]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            per_token_loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.contiguous().view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            per_token_loss = F.cross_entropy(logits, targets, reduction='none').view(b, t).mean(dim=0)\n",
    "\n",
    "        return logits, loss, per_token_loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens=6):\n",
    "        for _ in range(max_new_tokens):\n",
    "            x_cond = x[:, :]\n",
    "            logits, _, _ = self(x_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, 1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "block_size = train[features].shape[1]\n",
    "block_size_out = 1\n",
    "n_embd = 256\n",
    "n_hidden = 1024\n",
    "n_head = 256\n",
    "n_layer = 4\n",
    "n_vocab = 5\n",
    "lr = 1e-4\n",
    "n_epochs = 100\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.421953 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(block_size, block_size_out, n_embd, n_head, n_layer, n_vocab, n_hidden, dropout).to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 0, ..., 2, 2, 3],\n",
       "       [4, 2, 2, ..., 3, 2, 1],\n",
       "       [4, 4, 2, ..., 1, 2, 2],\n",
       "       ...,\n",
       "       [4, 4, 2, ..., 1, 2, 1],\n",
       "       [0, 2, 0, ..., 0, 2, 4],\n",
       "       [3, 0, 4, ..., 3, 2, 1]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[features].iloc[0:16].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 4, 0,  ..., 2, 2, 3],\n",
      "        [4, 2, 2,  ..., 3, 2, 1],\n",
      "        [4, 4, 2,  ..., 1, 2, 2],\n",
      "        ...,\n",
      "        [4, 4, 2,  ..., 1, 2, 1],\n",
      "        [0, 2, 0,  ..., 0, 2, 4],\n",
      "        [3, 0, 4,  ..., 3, 2, 1]], device='cuda:0')\n",
      "torch.int64\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 192.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 121\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_emb(x)\n\u001b[1;32m--> 121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 84\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(a))\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(a))\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viktor\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)\n\u001b[0;32m     35\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[1;32m---> 36\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:t, :t] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     38\u001b[0m w \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(w, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 192.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model(torch.tensor(train[features].iloc[0:16].values).to(device).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
